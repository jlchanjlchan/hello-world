# -*- coding: utf-8 -*-
"""
Created on Sat Apr 13 04:30:40 2019

@author: Jin Lung i Eric Ariño
"""

from bs4 import BeautifulSoup
import requests
import pandas as pd
import requests

URL_BASE = "https://www.habitaclia.com/alquiler-barcelona-"
#modificació de l'user agent
headers = {'User-Agent': 'Mozilla/5.0'}
MAX_PAGES = 10
counter = 0
d=[]
preus=[]

#control de les restriccions del ftixer robots.txt
response = requests.get('https://www.habitaclia.com/robots.txt')
robots = response.text
print("Fitxer robots.txt de https://www.habitaclia.com")
print("Verificació de restriccions:\n")
print(robots)

print("\nRealitzant scraping")

for i in range(1, MAX_PAGES):

    if i < 1:
        url = URL_BASE
    else:
        url = "%s%d.htm" % (URL_BASE, i)
        print(url)
        req = requests.get(url,headers=headers)
    # Comprobamos que la petición nos devuelve un Status Code = 200
        statusCode = req.status_code
        
        if statusCode == 200:
            
        # Pasamos el contenido HTML de la web a un objeto BeautifulSoup()
            html = BeautifulSoup(req.text, "html.parser")
            
        # Obtenemos todos los divs donde estan las entradas
            #entradas = html.find_all("list-item-content")
            panell = html.find(id="js-list")
            entradas = panell.find_all(class_="list-item-content")
            
        # Recorremos todas las entradas para extraer el título, autor y fecha
            for entrada in entradas:
                
                #Bucle pels titols dels anuncis                
                titols = entrada.find(class_= 'list-item-title').get_text()
                TipusImmoble = entrada.find(class_= 'list-item-title').get_text()
                             
                #Bucle pels preus dels anuncis
                tot_preus = panell.select(".list-item-content-second .font-2")
                preus = [pt.get_text() for pt in tot_preus]

                #Bucle per la localitzacio dels anuncis:
                barri = entrada.find(class_ = 'list-item-location').get_text()
                
                #Bucle per les descripcions dels anuncis:
                descripcions = entrada.find(class_= 'list-item-description').get_text()
                
                #Bucle pels metres dels anuncis:
                metres = entrada.find(class_= 'list-item-feature').get_text()
                
                #Bucle pels metres dels anuncis:
                lavabos = entrada.find(class_= 'list-item-feature').get_text()
                
                 #Bucle pels metres dels anuncis:
                habitacions = entrada.find(class_= 'list-item-feature').get_text()
            
            # Imprimo el Título, Autor y Fecha de las entradas

                d.append({
                       #count":counter,
                       "Titol": titols,
                       "TipusImmoble": TipusImmoble,
                       "Preu": preus[counter],
                       "Barri": barri,
                       "Metres": metres,
                       "Lavabo": lavabos ,
                       "Habitacions": habitacions,
                       "Descripcio": descripcions,
                      })
                taula=pd.DataFrame(d)
                taula['Metres']=taula.Metres.str.extract('(\d+)')
                taula['Lavabo']=taula.Lavabo.str.extract('.+(\d+) baño')
                taula['Habitacions']=taula.Habitacions.str.extract('.+(\d+) habitaciones')
                taula['Barri']=taula.Barri.str.extract('Barcelona - (.+)')
                taula['TipusImmoble']=taula.TipusImmoble.str.extract('Alquiler (.+?)\s')
                
                if counter==14:
                    counter=0
                else:
                    counter += 1
                    
    #print (counter,metres)    
        else:
        # Si ya no existe la página y me da un 400
            break 

taula.to_excel('habitaclia.xls')
taula.to_csv('habitaclia.csv')
